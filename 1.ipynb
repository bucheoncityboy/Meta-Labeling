{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd5c4ab-13db-4341-b26f-e480bbaa00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas_ta as ta\n",
    "import wikipedia as wp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ae3270-ef99-41d8-bc48-4182da44fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia에서 S&P 500 티커 목록 가져오기\n",
    "html = wp.page(\"List of S&P 500 companies\").html()\n",
    "sp500_df = pd.read_html(html)[0]\n",
    "tickers = sp500_df['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8484a71-b62e-4f78-a683-804855fc71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_download = tickers + ['^VIX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4984c88-612d-449c-828a-637e81ea249e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  504 of 504 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['EXE', 'GEHC', 'SOLV', 'GEV', 'CEG', 'KVUE', 'VLTO', 'COIN']: YFPricesMissingError('possibly delisted; no price data found  (1d 2015-01-01 -> 2020-12-31) (Yahoo error = \"Data doesn\\'t exist for startDate = 1420088400, endDate = 1609390800\")')\n",
      "['BF.B']: YFPricesMissingError('possibly delisted; no price data found  (1d 2015-01-01 -> 2020-12-31)')\n",
      "['BRK.B']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 다운로드 완료.\n"
     ]
    }
   ],
   "source": [
    "#데이터 다운로드\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2020-12-31'\n",
    "raw_data = yf.download(tickers_to_download, start=start_date, end=end_date, progress=True)\n",
    "\n",
    "print(\"데이터 다운로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0495f47-5c96-46ec-b7ea-da27f2be0a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Metadata:   3%|▋                      | 14/503 [00:07<04:18,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "# Open, Close, VIX 데이터만 선택\n",
    "data_subset = raw_data[['Open', 'Close']]\n",
    "vix_data = raw_data['Close']['^VIX'].rename('VIX_Close')\n",
    "\n",
    "# 데이터를 long-format(stacked)으로 변환\n",
    "stacked_data = data_subset.stack().reset_index()\n",
    "stacked_data.columns = ['Date', 'Stock', 'Open', 'Close']\n",
    "\n",
    "# VIX 데이터 병합\n",
    "stacked_data = pd.merge(stacked_data, vix_data, on='Date', how='left')\n",
    "\n",
    "# 섹터 및 시가총액 정보 가져오기 (API 호출이 많아 시간이 걸릴 수 있습니다)\n",
    "metadata = {}\n",
    "for ticker in tqdm(tickers, desc=\"Fetching Metadata\"):\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        metadata[ticker] = {\n",
    "            'Sector': info.get('sector', 'N/A'),\n",
    "            'Market_Cap': info.get('marketCap', np.nan)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get info for {ticker}: {e}\")\n",
    "        metadata[ticker] = {'Sector': 'N/A', 'Market_Cap': np.nan}\n",
    "\n",
    "metadata_df = pd.DataFrame.from_dict(metadata, orient='index').reset_index().rename(columns={'index': 'Stock'})\n",
    "\n",
    "# 주가 데이터에 메타데이터 병합\n",
    "full_df = pd.merge(stacked_data, metadata_df, on='Stock', how='left')\n",
    "\n",
    "# 데이터 클리닝\n",
    "full_df.dropna(subset=['Open', 'Close', 'Sector', 'Market_Cap'], inplace=True)\n",
    "full_df = full_df[full_df['Sector'] != 'N/A']\n",
    "\n",
    "print(f\"전처리 후 데이터 형태: {full_df.shape}\")\n",
    "print(\"데이터 샘플:\")\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f2388c-1444-4154-b082-d7f316728434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA 피처 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 45 EMA Features:   0%|                        | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Generating 45 EMA Features:   2%|▎               | 1/45 [00:00<00:26,  1.63it/s]\u001b[A\n",
      "Generating 45 EMA Features:   4%|▋               | 2/45 [00:01<00:24,  1.73it/s]\u001b[A\n",
      "Generating 45 EMA Features:   7%|█               | 3/45 [00:01<00:24,  1.69it/s]\u001b[A\n",
      "Generating 45 EMA Features:   9%|█▍              | 4/45 [00:02<00:23,  1.71it/s]\u001b[A\n",
      "Generating 45 EMA Features:  11%|█▊              | 5/45 [00:02<00:22,  1.75it/s]\u001b[A\n",
      "Generating 45 EMA Features:  13%|██▏             | 6/45 [00:03<00:23,  1.67it/s]\u001b[A\n",
      "Generating 45 EMA Features:  16%|██▍             | 7/45 [00:04<00:22,  1.73it/s]\u001b[A\n",
      "Generating 45 EMA Features:  18%|██▊             | 8/45 [00:04<00:21,  1.75it/s]\u001b[A\n",
      "Generating 45 EMA Features:  20%|███▏            | 9/45 [00:05<00:20,  1.77it/s]\u001b[A\n",
      "Generating 45 EMA Features:  22%|███▎           | 10/45 [00:05<00:19,  1.81it/s]\u001b[A\n",
      "Generating 45 EMA Features:  24%|███▋           | 11/45 [00:06<00:18,  1.83it/s]\u001b[A\n",
      "Generating 45 EMA Features:  27%|████           | 12/45 [00:06<00:17,  1.88it/s]\u001b[A\n",
      "Generating 45 EMA Features:  29%|████▎          | 13/45 [00:07<00:16,  1.92it/s]\u001b[A\n",
      "Generating 45 EMA Features:  31%|████▋          | 14/45 [00:07<00:16,  1.93it/s]\u001b[A\n",
      "Generating 45 EMA Features:  33%|█████          | 15/45 [00:08<00:15,  1.89it/s]\u001b[A\n",
      "Generating 45 EMA Features:  36%|█████▎         | 16/45 [00:08<00:15,  1.86it/s]\u001b[AException ignored in: <function tqdm.__del__ at 0x11c2bf880>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/anaconda3/envs/t1/lib/python3.13/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/Library/anaconda3/envs/t1/lib/python3.13/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "\n",
      "Generating 45 EMA Features:  38%|█████▋         | 17/45 [00:09<00:19,  1.45it/s]\u001b[A\n",
      "Generating 45 EMA Features:  40%|██████         | 18/45 [00:10<00:17,  1.52it/s]\u001b[A\n",
      "Generating 45 EMA Features:  42%|██████▎        | 19/45 [00:11<00:17,  1.50it/s]\u001b[A\n",
      "Generating 45 EMA Features:  44%|██████▋        | 20/45 [00:11<00:15,  1.59it/s]\u001b[A\n",
      "Generating 45 EMA Features:  47%|███████        | 21/45 [00:12<00:14,  1.67it/s]\u001b[A\n",
      "Generating 45 EMA Features:  49%|███████▎       | 22/45 [00:12<00:13,  1.72it/s]\u001b[A\n",
      "Generating 45 EMA Features:  51%|███████▋       | 23/45 [00:13<00:12,  1.70it/s]\u001b[A\n",
      "Generating 45 EMA Features:  53%|████████       | 24/45 [00:13<00:11,  1.78it/s]\u001b[A\n",
      "Generating 45 EMA Features:  56%|████████▎      | 25/45 [00:14<00:10,  1.83it/s]\u001b[A\n",
      "Generating 45 EMA Features:  58%|████████▋      | 26/45 [00:14<00:10,  1.82it/s]\u001b[A\n",
      "Generating 45 EMA Features:  60%|█████████      | 27/45 [00:15<00:09,  1.83it/s]\u001b[A\n",
      "Generating 45 EMA Features:  62%|█████████▎     | 28/45 [00:16<00:09,  1.79it/s]\u001b[A\n",
      "Generating 45 EMA Features:  64%|█████████▋     | 29/45 [00:16<00:08,  1.80it/s]\u001b[A\n",
      "Generating 45 EMA Features:  67%|██████████     | 30/45 [00:17<00:08,  1.82it/s]\u001b[A\n",
      "Generating 45 EMA Features:  69%|██████████▎    | 31/45 [00:17<00:07,  1.84it/s]\u001b[A\n",
      "Generating 45 EMA Features:  71%|██████████▋    | 32/45 [00:18<00:07,  1.82it/s]\u001b[A\n",
      "Generating 45 EMA Features:  73%|███████████    | 33/45 [00:18<00:06,  1.82it/s]\u001b[A\n",
      "Generating 45 EMA Features:  76%|███████████▎   | 34/45 [00:19<00:05,  1.86it/s]\u001b[A\n",
      "Generating 45 EMA Features:  78%|███████████▋   | 35/45 [00:19<00:05,  1.76it/s]\u001b[A\n",
      "Generating 45 EMA Features:  80%|████████████   | 36/45 [00:20<00:05,  1.79it/s]\u001b[A\n",
      "Generating 45 EMA Features:  82%|████████████▎  | 37/45 [00:21<00:04,  1.71it/s]\u001b[A\n",
      "Generating 45 EMA Features:  84%|████████████▋  | 38/45 [00:21<00:04,  1.74it/s]\u001b[A\n",
      "Generating 45 EMA Features:  87%|█████████████  | 39/45 [00:22<00:03,  1.66it/s]\u001b[A\n",
      "Generating 45 EMA Features:  89%|█████████████▎ | 40/45 [00:22<00:02,  1.72it/s]\u001b[A\n",
      "Generating 45 EMA Features:  91%|█████████████▋ | 41/45 [00:23<00:02,  1.75it/s]\u001b[A\n",
      "Generating 45 EMA Features:  93%|██████████████ | 42/45 [00:24<00:01,  1.74it/s]\u001b[A\n",
      "Generating 45 EMA Features:  96%|██████████████▎| 43/45 [00:24<00:01,  1.74it/s]\u001b[A\n",
      "Generating 45 EMA Features:  98%|██████████████▋| 44/45 [00:25<00:00,  1.76it/s]\u001b[A\n",
      "Generating 45 EMA Features: 100%|███████████████| 45/45 [00:25<00:00,  1.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처 엔지니어링 완료:\n",
      "            Date Stock       Open      Close  VIX_Close      Sector  \\\n",
      "46173 2015-05-27     A  38.859116  39.171680      13.27  Healthcare   \n",
      "46640 2015-05-28     A  39.024577  38.381062      13.31  Healthcare   \n",
      "47107 2015-05-29     A  38.417859  37.866272      13.84  Healthcare   \n",
      "47574 2015-06-01     A  38.086902  37.618053      13.97  Healthcare   \n",
      "48041 2015-06-02     A  37.636437  37.792721      14.24  Healthcare   \n",
      "\n",
      "         Market_Cap  Stock_Return  Abnormal_Return   EMA_3_1  ...  EMA_19_13  \\\n",
      "46173  3.518802e+10      0.013076         0.001648  0.004184  ...   0.000781   \n",
      "46640  3.518802e+10     -0.020183        -0.019030 -0.008107  ...   0.000141   \n",
      "47107  3.518802e+10     -0.013413        -0.016788 -0.010821  ...  -0.000926   \n",
      "47574  3.518802e+10     -0.006555        -0.011470 -0.008728  ...  -0.002017   \n",
      "48041  3.518802e+10      0.004643         0.010132 -0.002067  ...  -0.002639   \n",
      "\n",
      "       EMA_17_15  EMA_19_15  EMA_19_17     VIX_20    VIX_100  VIX_Feature  \\\n",
      "46173   0.000250   0.000474   0.000224  13.204442  15.334600    -2.130158   \n",
      "46640   0.000045   0.000107   0.000062  13.214496  15.294509    -2.080013   \n",
      "47107  -0.000301  -0.000517  -0.000216  13.274067  15.265707    -1.991639   \n",
      "47574  -0.000657  -0.001166  -0.000509  13.340347  15.240049    -1.899702   \n",
      "48041  -0.000863  -0.001552  -0.000689  13.426028  15.220246    -1.794218   \n",
      "\n",
      "       Stock_Vol_5  Stock_Vol_20  Stock_Unique_Volatility  \n",
      "46173     0.004533      0.005990                -0.001458  \n",
      "46640     0.010596      0.007905                 0.002690  \n",
      "47107     0.010218      0.008654                 0.001565  \n",
      "47574     0.008383      0.008539                -0.000156  \n",
      "48041     0.012818      0.009261                 0.003557  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "    df.sort_values(by=['Stock', 'Date'], inplace=True)\n",
    "    df['Stock_Return'] = df.groupby('Stock')['Close'].pct_change().fillna(0)\n",
    "    \n",
    "    # 섹터별 평균 수익률 및 비정상 수익률 [cite: 70]\n",
    "    sector_return = df.groupby(['Date', 'Sector'])['Stock_Return'].transform('mean')\n",
    "    df['Abnormal_Return'] = df['Stock_Return'] - sector_return\n",
    "    \n",
    "    # EMA 피처 [cite: 121]\n",
    "    # 45개\n",
    "    print(\"EMA 피처 생성 중...\")\n",
    "    short_windows = [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "    long_windows = [3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "    combinations = []\n",
    "    for s in short_windows:\n",
    "        for l in long_windows:\n",
    "        # 롱윈도우가 숏윈도우보다 최소 2일 길어야 한다는 조건\n",
    "            if l >= s + 2:\n",
    "                combinations.append((s, l))\n",
    "    for s, l in tqdm(combinations, desc=\"Generating 45 EMA Features\"):\n",
    "        col_name = f'EMA_{l}_{s}'\n",
    "        ema_short = df.groupby('Stock')['Close'].transform(lambda x: ta.ema(x, length=s))\n",
    "        ema_long = df.groupby('Stock')['Close'].transform(lambda x: ta.ema(x, length=l))\n",
    "        df[col_name] = (ema_short - ema_long) / ema_long\n",
    "\n",
    "    # VIX 피처 [cite: 140]\n",
    "    df['VIX_20'] = ta.ema(df['VIX_Close'], length=20)\n",
    "    df['VIX_100'] = ta.ema(df['VIX_Close'], length=100)\n",
    "    df['VIX_Feature'] = df['VIX_20'] - df['VIX_100']\n",
    "\n",
    "    # 개별 주식 변동성 피처 [cite: 143]\n",
    "    df['Stock_Vol_5'] = df.groupby('Stock')['Abnormal_Return'].transform(lambda x: x.ewm(span=5).std())\n",
    "    df['Stock_Vol_20'] = df.groupby('Stock')['Abnormal_Return'].transform(lambda x: x.ewm(span=20).std())\n",
    "    df['Stock_Unique_Volatility'] = df['Stock_Vol_5'] - df['Stock_Vol_20']\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "processed_df = feature_engineering(full_df.copy())\n",
    "print(\"피처 엔지니어링 완료:\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5cb390-3d67-455f-98e9-d08fd83f5448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Binning 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning features: 100%|█████████████████████████| 48/48 [01:43<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터: (590344, 61), 테스트 데이터: (123384, 61)\n"
     ]
    }
   ],
   "source": [
    "# ===== [수정된 셀 7] =====\n",
    "\n",
    "def quantile_binning(df):\n",
    "    # 'EMA_'가 포함된 모든 컬럼 이름을 가져옵니다.\n",
    "    ema_cols = [col for col in df.columns if 'EMA_' in col]\n",
    "    # Market_Cap과 Stock_Unique_Volatility는 VIX와 함께 별도 처리하거나 논문 방식에 따라 처리합니다.\n",
    "    # 여기서는 논문의 핵심인 EMA 피처 이산화에 집중합니다.\n",
    "    features_to_bin = ema_cols # + ['Stock_Unique_Volatility', 'Market_Cap']\n",
    "    \n",
    "    print(\"Quantile Binning 중 (EMA Features)...\")\n",
    "    for col in tqdm(features_to_bin, desc=\"Binning EMA features\"):\n",
    "        # <<수정 포인트 1>>: 그룹화 기준에 'Sector'를 추가하여 섹터 내 분위수를 계산합니다.\n",
    "        df[col] = df.groupby(['Date', 'Sector'])[col].transform(\n",
    "            lambda x: pd.qcut(x, 5, labels=False, duplicates='drop')\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# VIX 피처는 아래 다음 셀에서 별도로 처리하므로, 여기서는 EMA만 처리합니다.\n",
    "binned_df = quantile_binning(processed_df.copy())\n",
    "\n",
    "# 타겟 변수 생성\n",
    "binned_df['Target'] = (binned_df['Abnormal_Return'] > 0).astype(int)\n",
    "\n",
    "# 데이터 분할\n",
    "train_df = binned_df[pd.to_datetime(binned_df['Date']) < '2020-01-01']\n",
    "test_df = binned_df[pd.to_datetime(binned_df['Date']) >= '2020-01-01']\n",
    "\n",
    "print(f\"학습 데이터: {train_df.shape}, 테스트 데이터: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38087992-1e6f-4eee-a7c7-ee7affd582d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== [새로 추가할 셀] =====\n",
    "\n",
    "print(\"VIX 피처 롤링 윈도우 이산화 중...\")\n",
    "\n",
    "# 1. 날짜별 고유 VIX 값을 추출하여 시계열 데이터 생성\n",
    "vix_daily = binned_df[['Date', 'VIX_Close']].drop_duplicates().set_index('Date').sort_index()\n",
    "\n",
    "# 2. 30일 롤링 윈도우를 적용하여 VIX 값을 5분위수로 변환하는 함수\n",
    "def assign_rolling_quintile(series):\n",
    "    current_value = series.iloc[-1]\n",
    "    try:\n",
    "        bins = pd.qcut(series, 5, retbins=True, duplicates='drop')[1]\n",
    "        return pd.cut([current_value], bins=bins, labels=False, include_lowest=True)[0]\n",
    "    except (ValueError, IndexError):\n",
    "        return np.nan\n",
    "\n",
    "# 3. 함수 적용\n",
    "vix_daily['VIX_quintile'] = vix_daily['VIX_Close'].rolling(window=30, min_periods=5).apply(assign_rolling_quintile, raw=False)\n",
    "\n",
    "# 4. 계산된 VIX 분위수 값을 원래 데이터프레임에 병합\n",
    "# 기존의 VIX_Feature 대신 새로운 VIX_quintile을 사용합니다.\n",
    "train_df = pd.merge(train_df, vix_daily[['VIX_quintile']], on='Date', how='left')\n",
    "test_df = pd.merge(test_df, vix_daily[['VIX_quintile']], on='Date', how='left')\n",
    "\n",
    "# 병합 후 생길 수 있는 결측치 처리\n",
    "train_df.dropna(subset=['VIX_quintile'], inplace=True)\n",
    "test_df.dropna(subset=['VIX_quintile'], inplace=True)\n",
    "train_df['VIX_quintile'] = train_df['VIX_quintile'].astype(int)\n",
    "test_df['VIX_quintile'] = test_df['VIX_quintile'].astype(int)\n",
    "\n",
    "\n",
    "print(\"VIX 피처 이산화 및 병합 완료.\")\n",
    "print(\"VIX_quintile 컬럼이 추가되었습니다.\")\n",
    "print(train_df[['Date', 'VIX_Close', 'VIX_quintile']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b9e10b-a233-4b8b-9e4e-b2bde74e2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SSFI Feature Selection:  44%|████████▍          | 20/45 [22:53<28:37, 68.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(feature_scores.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[32m10\u001b[39m]\n\u001b[32m     30\u001b[39m ema_features = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m binned_df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mEMA_\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m top_10_features = ssfi_feature_selection(train_df, ema_features)\n\u001b[32m     32\u001b[39m selected_features = [f[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m top_10_features]\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m상위 10개 피처:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mssfi_feature_selection\u001b[39m\u001b[34m(df, features)\u001b[39m\n\u001b[32m     15\u001b[39m sw_train, sw_val = sample_weights.iloc[train_idx], sample_weights.iloc[val_idx]\n\u001b[32m     17\u001b[39m model = BaggingClassifier(\n\u001b[32m     18\u001b[39m     DecisionTreeClassifier(), \n\u001b[32m     19\u001b[39m     n_estimators=\u001b[32m100\u001b[39m, \u001b[38;5;66;03m# 계산 시간을 위해 estimators 수 조정\u001b[39;00m\n\u001b[32m     20\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model.fit(X_train, y_train, sample_weight=sw_train)\n\u001b[32m     23\u001b[39m preds = model.predict(X_val)\n\u001b[32m     24\u001b[39m scores.append(matthews_corrcoef(y_val, preds, sample_weight=sw_val))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:389\u001b[39m, in \u001b[36mBaseBagging.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[32m    379\u001b[39m X, y = validate_data(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    381\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    386\u001b[39m     multi_output=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    387\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(\n\u001b[32m    390\u001b[39m     X,\n\u001b[32m    391\u001b[39m     y,\n\u001b[32m    392\u001b[39m     max_samples=\u001b[38;5;28mself\u001b[39m.max_samples,\n\u001b[32m    393\u001b[39m     sample_weight=sample_weight,\n\u001b[32m    394\u001b[39m     **fit_params,\n\u001b[32m    395\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:547\u001b[39m, in \u001b[36mBaseBagging._fit\u001b[39m\u001b[34m(self, X, y, max_samples, max_depth, check_input, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    544\u001b[39m seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n\u001b[32m    545\u001b[39m \u001b[38;5;28mself\u001b[39m._seeds = seeds\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m all_results = Parallel(\n\u001b[32m    548\u001b[39m     n_jobs=n_jobs, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **\u001b[38;5;28mself\u001b[39m._parallel_args()\n\u001b[32m    549\u001b[39m )(\n\u001b[32m    550\u001b[39m     delayed(_parallel_build_estimators)(\n\u001b[32m    551\u001b[39m         n_estimators[i],\n\u001b[32m    552\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    553\u001b[39m         X,\n\u001b[32m    554\u001b[39m         y,\n\u001b[32m    555\u001b[39m         seeds[starts[i] : starts[i + \u001b[32m1\u001b[39m]],\n\u001b[32m    556\u001b[39m         total_n_estimators,\n\u001b[32m    557\u001b[39m         verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    558\u001b[39m         check_input=check_input,\n\u001b[32m    559\u001b[39m         fit_params=routed_params.estimator.fit,\n\u001b[32m    560\u001b[39m     )\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_jobs)\n\u001b[32m    562\u001b[39m )\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ += \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    566\u001b[39m     itertools.chain.from_iterable(t[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[32m    567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = func(*args, **kwargs)\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/ensemble/_bagging.py:187\u001b[39m, in \u001b[36m_parallel_build_estimators\u001b[39m\u001b[34m(n_estimators, ensemble, X, y, seeds, total_n_estimators, verbose, check_input, fit_params)\u001b[39m\n\u001b[32m    185\u001b[39m     fit_params_[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = curr_sample_weight\n\u001b[32m    186\u001b[39m     X_ = X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     estimator_fit(X_, y, **fit_params_)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# cannot use sample_weight, so use indexing\u001b[39;00m\n\u001b[32m    190\u001b[39m     y_ = _safe_indexing(y, indices)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/tree/_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28msuper\u001b[39m()._fit(\n\u001b[32m   1025\u001b[39m         X,\n\u001b[32m   1026\u001b[39m         y,\n\u001b[32m   1027\u001b[39m         sample_weight=sample_weight,\n\u001b[32m   1028\u001b[39m         check_input=check_input,\n\u001b[32m   1029\u001b[39m     )\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/anaconda3/envs/t1/lib/python3.13/site-packages/sklearn/tree/_classes.py:307\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    305\u001b[39m     classes_k, y_encoded[:, k] = np.unique(y[:, k], return_inverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_.append(classes_k)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_.append(classes_k.shape[\u001b[32m0\u001b[39m])\n\u001b[32m    308\u001b[39m y = y_encoded\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def ssfi_feature_selection(df, features):\n",
    "    sample_weights = np.abs(df['Abnormal_Return'])\n",
    "    y = df['Target']\n",
    "    cv = KFold(n_splits=10, shuffle=False)\n",
    "    \n",
    "    feature_scores = {}\n",
    "    for feature in tqdm(features, desc=\"SSFI Feature Selection\"):\n",
    "        X = df[[feature]]\n",
    "        scores = []\n",
    "        \n",
    "        # CV를 통한 성능 측정\n",
    "        for train_idx, val_idx in cv.split(X):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            sw_train, sw_val = sample_weights.iloc[train_idx], sample_weights.iloc[val_idx]\n",
    "            \n",
    "            model = BaggingClassifier(\n",
    "                DecisionTreeClassifier(), \n",
    "                n_estimators=100, # 계산 시간을 위해 estimators 수 조정\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_train, y_train, sample_weight=sw_train)\n",
    "            preds = model.predict(X_val)\n",
    "            scores.append(matthews_corrcoef(y_val, preds, sample_weight=sw_val))\n",
    "        \n",
    "        feature_scores[feature] = np.mean(scores)\n",
    "\n",
    "    return sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "ema_features = [col for col in binned_df.columns if 'EMA_' in col]\n",
    "top_10_features = ssfi_feature_selection(train_df, ema_features)\n",
    "selected_features = [f[0] for f in top_10_features]\n",
    "\n",
    "print(\"\\n상위 10개 피처:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d88fe9-c1cd-49e5-8636-e6eb7a89b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== [수정된 셀 9] =====\n",
    "\n",
    "# <<수정 포인트 2>>: Sector를 정수 레이블로 인코딩합니다.\n",
    "train_df['Sector_encoded'] = pd.factorize(train_df['Sector'])[0]\n",
    "test_df['Sector_encoded'] = pd.factorize(test_df['Sector'])[0]\n",
    "\n",
    "\n",
    "# additional_features에 새로 만든 VIX_quintile과 Sector_encoded를 사용합니다.\n",
    "# 기존 VIX_Feature와 Market_Cap은 논문 핵심 피처가 아니므로 일단 제외하거나 필요시 추가합니다.\n",
    "additional_features = ['Sector_encoded', 'VIX_quintile', 'Stock_Unique_Volatility']\n",
    "\n",
    "# 피처셋 정의\n",
    "train_features = train_df[selected_features + additional_features]\n",
    "test_features = test_df[selected_features + additional_features]\n",
    "\n",
    "# 학습/테스트 데이터의 컬럼을 동일하게 맞춤\n",
    "train_labels, test_labels = train_features.align(test_features, join='inner', axis=1, fill_value=0)\n",
    "\n",
    "X_train_primary = train_labels[selected_features]\n",
    "X_test_primary = test_labels[selected_features]\n",
    "\n",
    "X_train_add = train_labels[[col for col in additional_features if col in train_labels.columns]]\n",
    "X_test_add = test_labels[[col for col in additional_features if col in test_labels.columns]]\n",
    "\n",
    "y_train = train_df.loc[train_labels.index, 'Target']\n",
    "y_test = test_df.loc[test_labels.index, 'Target']\n",
    "sw_train = np.abs(train_df.loc[train_labels.index, 'Abnormal_Return'])\n",
    "sw_test = np.abs(test_df.loc[test_labels.index, 'Abnormal_Return'])\n",
    "\n",
    "\n",
    "# <<수정 포인트 3>>: 모델 파라미터를 min_weight_fraction_leaf로 변경합니다.\n",
    "rf_params = {\n",
    "    'n_estimators': 200, \n",
    "    'max_features': 0.5, \n",
    "    'min_weight_fraction_leaf': 0.001,  # 샘플 개수가 아닌 가중치 합의 비율을 기준으로 리프 노드 결정\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# 1. Primary Model 학습\n",
    "print(\"Primary Model 학습 중...\")\n",
    "primary_model = RandomForestClassifier(**rf_params)\n",
    "primary_model.fit(X_train_primary, y_train, sample_weight=sw_train)\n",
    "primary_preds_train = primary_model.predict(X_train_primary)\n",
    "\n",
    "# Meta Target 생성\n",
    "meta_target_train = (primary_preds_train == y_train).astype(int)\n",
    "\n",
    "# 2. Meta Model 1 (Regimes Only) 학습\n",
    "print(\"Meta Model 1 학습 중...\")\n",
    "meta_model_1 = RandomForestClassifier(**rf_params)\n",
    "meta_model_1.fit(X_train_add, meta_target_train, sample_weight=sw_train)\n",
    "\n",
    "# 3. Meta Model 2 (Regimes + X) 학습\n",
    "print(\"Meta Model 2 학습 중...\")\n",
    "meta_model_2 = RandomForestClassifier(**rf_params)\n",
    "meta_model_2.fit(train_labels, meta_target_train, sample_weight=sw_train)\n",
    "\n",
    "# 4. Non-Meta Model 학습\n",
    "print(\"Non-Meta Model 학습 중...\")\n",
    "non_meta_model = RandomForestClassifier(**rf_params)\n",
    "non_meta_model.fit(train_labels, y_train, sample_weight=sw_train)\n",
    "\n",
    "print(\"\\n모든 모델 학습 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6066c9-dc1d-45d5-ac51-d9d956a2623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, sample_weight):\n",
    "    mcc = matthews_corrcoef(y_true, y_pred, sample_weight=sample_weight)\n",
    "    accuracy = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "    precision_1 = precision_score(y_true, y_pred, sample_weight=sample_weight, pos_label=1)\n",
    "    precision_0 = precision_score(y_true, y_pred, sample_weight=sample_weight, pos_label=0)\n",
    "    return {\n",
    "        'Matthews': mcc, \n",
    "        'Accuracy': accuracy, \n",
    "        'Precision 1': precision_1, \n",
    "        'Precision 0': precision_0\n",
    "    }\n",
    "\n",
    "print(\"--- OOS 평가 결과 ---\")\n",
    "\n",
    "# Primary Model 평가\n",
    "primary_preds_test = primary_model.predict(X_test_primary)\n",
    "primary_results = evaluate_model(y_test, primary_preds_test, sw_test)\n",
    "print(\"\\nPrimary Model 결과:\\n\", pd.Series(primary_results))\n",
    "\n",
    "# Non-Meta Model 평가\n",
    "non_meta_preds_test = non_meta_model.predict(test_labels)\n",
    "non_meta_results = evaluate_model(y_test, non_meta_preds_test, sw_test)\n",
    "print(\"\\nNon-Meta Model 결과:\\n\", pd.Series(non_meta_results))\n",
    "\n",
    "# Meta-labeling 적용 후 평가\n",
    "meta_preds_1 = meta_model_1.predict(X_test_add)\n",
    "meta_preds_2 = meta_model_2.predict(test_labels)\n",
    "\n",
    "# Meta Model 1\n",
    "y_test_meta1 = y_test[meta_preds_1 == 1]\n",
    "primary_preds_meta1 = pd.Series(primary_preds_test, index=y_test.index)[meta_preds_1 == 1]\n",
    "sw_test_meta1 = sw_test[meta_preds_1 == 1]\n",
    "if not y_test_meta1.empty:\n",
    "    meta1_results = evaluate_model(y_test_meta1, primary_preds_meta1, sw_test_meta1)\n",
    "    print(\"\\nMeta Model 1 ( 필터링 후) 결과:\\n\", pd.Series(meta1_results))\n",
    "\n",
    "# Meta Model 2\n",
    "y_test_meta2 = y_test[meta_preds_2 == 1]\n",
    "primary_preds_meta2 = pd.Series(primary_preds_test, index=y_test.index)[meta_preds_2 == 1]\n",
    "sw_test_meta2 = sw_test[meta_preds_2 == 1]\n",
    "if not y_test_meta2.empty:\n",
    "    meta2_results = evaluate_model(y_test_meta2, primary_preds_meta2, sw_test_meta2)\n",
    "    print(\"\\nMeta Model 2 ( 필터링 후) 결과:\\n\", pd.Series(meta2_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
